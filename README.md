# GML_NN: Librer√≠a de Redes Neuronales en C

[![Language](https://img.shields.io/badge/Language-C-blue.svg)](https://en.wikipedia.org/wiki/C_(programming_language))
[![Environment](https://img.shields.io/badge/Environment-GNU/Linux-green.svg)](https://www.gnu.org/linux/)

Este repositorio contiene **GML_NN**, una librer√≠a implementada desde cero en **C est√°ndar** para la creaci√≥n, entrenamiento y uso de **Redes Neuronales Artificiales** de tipo **Perceptr√≥n Multicapa (MLP)**. El proyecto fue desarrollado como Trabajo Fin de Grado (TFG) en la Escuela T√©cnica Superior de Ingenieros Inform√°ticos de la Universidad Polit√©cnica de Madrid.

El objetivo principal es ofrecer una herramienta en C que permita describir, entrenar mediante **retropropagaci√≥n** (`backpropagation`) y utilizar redes neuronales para resolver problemas de clasificaci√≥n, predicci√≥n y reconocimiento de patrones.

### üìÑ Documentaci√≥n Detallada

Este repositorio incluye:
* La **memoria completa del TFG**, donde se documenta en profundidad todo el proceso de dise√±o, la base te√≥rica (desde el Perceptr√≥n hasta la retropropagaci√≥n) y las decisiones de implementaci√≥n. Puedes consultarla en el repositorio oficial de la UPM: [Archivo Digital UPM](https://oa.upm.es/82476/). Para un an√°lisis exhaustivo de los algoritmos, las **funciones de activaci√≥n**, las **funciones de error**, los **optimizadores** y las **estructuras de datos** utilizadas, se recomienda consultar dicho documento.
* **Manuales de uso** detallados para la librer√≠a principal (`gml_nn.h`) y sus m√≥dulos auxiliares (`matrix.h`, `data_handler.h`) en la carpeta `manual/`. Estos manuales proporcionan una gu√≠a r√°pida sobre la interfaz de la librer√≠a y ejemplos de c√≥digo.

---

## 1. Caracter√≠sticas Principales de GML_NN ‚ú®

* **Creaci√≥n Flexible de Redes:** Permite definir arquitecturas MLP con un n√∫mero arbitrario de capas y neuronas por capa.
* **Entrenamiento Supervisado:** Implementa el algoritmo de **retropropagaci√≥n** (`backpropagation`) para ajustar los pesos.
* **Variedad de Funciones de Activaci√≥n:** Incluye funciones comunes como Sigmoide, Tanh, Sigmoide Optimizado (LeCun), ReLU, Leaky ReLU y Softplus.
* **Funciones de Error Configurables:** Implementa Error Cuadr√°tico Medio (MSE) y su variante dividida por 2.
* **Optimizadores:** Soporta Descenso de Gradiente Estoc√°stico (SGD), por lotes (Batch) y mini-lotes, con opci√≥n de a√±adir **Momento**.
* **Personalizaci√≥n:** Permite definir y usar **funciones de activaci√≥n y de error personalizadas** por el usuario (calculando derivadas mediante diferencias finitas).
* **Gesti√≥n de Modelos:** Funcionalidad para **guardar** el estado de una red entrenada en un fichero `.nn` y **cargarla** posteriormente.
* **Modularidad:** Incluye m√≥dulos auxiliares para operaciones con **matrices (`matrix.h`)** y **manejo de datos (`data_handler.h`)** desde ficheros `.csv`.

---

## 2. Arquitectura y Detalles T√©cnicos üõ†Ô∏è

La librer√≠a est√° estructurada en torno a dos `struct` principales:

1.  `layer`: Representa una capa de la red, almacenando sus **pesos (`W`)**, matrices auxiliares para el entrenamiento (`dW`, `vw`, etc.) y la salida (`out`). Los pesos y sesgos se gestionan conjuntamente en una √∫nica matriz por capa para optimizar operaciones.
2.  `neural_net`: Contiene la configuraci√≥n global de la red (tasa de aprendizaje, tasa de decadencia para momento, semilla aleatoria, tama√±o de lote, funci√≥n de error) y un array de punteros a las `layer` que la componen.



El **entrenamiento** se basa en el c√°lculo del gradiente del error respecto a cada peso, propagando este error desde la capa de salida hacia la de entrada. La implementaci√≥n modular permite manejar diferentes funciones de activaci√≥n y error sin necesidad de reescribir todo el algoritmo de retropropagaci√≥n. Se utiliza la **regla delta generalizada**, adaptada para trabajar con punteros a funciones para las derivadas.

---

## 3. M√≥dulos Auxiliares

* **`matrix.h` / `matrix.c`**: Implementa una estructura `matrix` y operaciones fundamentales (creaci√≥n, liberaci√≥n, acceso, suma, resta, producto matricial, producto escalar) necesarias para el *feedforward* y *backpropagation*.
* **`data_handler.h` / `data_handler.c`**: Facilita la carga de datos desde ficheros `.csv`, separaci√≥n en conjuntos de entrada/salida, divisi√≥n en entrenamiento/prueba, barajado (Fisher-Yates), normalizaci√≥n (MinMax) y transformaci√≥n de clases enteras a formato binario (*one-hot encoding* adaptado).

---

## 4. C√≥mo Compilar y Ejecutar Ejemplos üöÄ

La librer√≠a no tiene dependencias externas m√°s all√° del compilador C (gcc recomendado) y la librer√≠a matem√°tica (`-lm`). Para compilar un ejemplo (ej. `xor.c`), puedes usar un `Makefile` similar al siguiente:

```makefile
CC = gcc
CFLAGS = -Wall -Wextra -g -I. # A√±ade -I. para incluir cabeceras locales
LDFLAGS = -lm
SRC = matrix.c data_handler.c gml_nn.c tu_ejemplo.c # Reemplaza tu_ejemplo.c
TARGET = tu_ejecutable # Reemplaza tu_ejecutable

$(TARGET): $(SRC)
	$(CC) $(CFLAGS) -o $@ $^ $(LDFLAGS)

clean:
	rm -f $(TARGET)
```

1.  Guarda el c√≥digo anterior como `Makefile`.
2.  Reemplaza `tu_ejemplo.c` y `tu_ejecutable`.
3.  Ejecuta `make` en la terminal para compilar.
4.  Ejecuta `./tu_ejecutable` para correr el programa.

El repositorio incluye ejemplos pr√°cticos:
* Resoluci√≥n de puertas l√≥gicas (XOR).
* Funciones l√≥gicas m√°s complejas.
* Clasificaci√≥n de puntos en 2D (incluyendo datos no lineales como espirales).
* Predicci√≥n con datos reales (Diabetes, VIH).
* Reconocimiento de d√≠gitos MNIST (con resultados limitados).

---

## 5. Resultados y Limitaciones üìä

La librer√≠a demuestra ser capaz de resolver problemas de clasificaci√≥n lineal y no lineal con √©xito en varios conjuntos de datos, logrando tasas de acierto comparables a las de otras herramientas en los datasets de Diabetes y VIH.

Sin embargo, en el problema de reconocimiento de d√≠gitos **MNIST**, el rendimiento fue limitado (en torno al 58% en el mejor caso). La memoria del TFG identifica posibles causas:
* Uso exclusivo de MLP (las CNNs suelen ser m√°s adecuadas para im√°genes).
* Conectividad total entre capas.
* Normalizaci√≥n MinMax podr√≠a no ser ideal para MNIST.
* Falta de optimizadores m√°s avanzados (como Adam, RMSProp).
* Falta de funciones de error m√°s espec√≠ficas para clasificaci√≥n multiclase (como Cross-Entropy).

---

## 6. Futuras L√≠neas de Trabajo üîÆ

La memoria del TFG sugiere varias mejoras posibles:
* Refactorizar el uso de punteros a funciones para optimizar el rendimiento.
* Expandir el cat√°logo de funciones de activaci√≥n y error (ej. Cross-Entropy).
* Implementar optimizadores avanzados (Adam, RMSProp).
* Permitir eliminar conexiones entre neuronas (redes no totalmente conectadas).
* A√±adir soporte para capas convolucionales (CNNs) y pooling.
* Mejorar las herramientas de visualizaci√≥n.
